---
title: 'homework'
author: Can Wang
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# HW0

## Question 1

Go through "R for Beginners" if you are not familiar with R programming.

## Answer 1

Finished.

##Question 2
Use knitr to produce 3 examples in the book.
The 1st example should contain texts and at least one figure. 
The 2nd example should contains texts and at least one table. 
The 3rd example should contain at least a couple of LaTeX formulas.

##Answer 2
###The 1st example should contain texts and at least one figure. 

```{r}
Y= c(10.98, 11.13, 12.51, 8.40, 9.27, 8.73, 6.36, 8.50,
7.82, 9.14, 8.24, 12.19, 11.88,9.57, 10.94, 9.58, 10.09,
8.11, 6.83, 8.88, 7.68, 8.47, 8.86, 10.36, 11.08)
X =c(35.3, 29.7, 30.8, 58.8, 61.4, 71.3, 74.4, 76.7, 70.7,
57.5, 46.4, 28.9, 28.1, 39.1, 46.8, 48.5, 59.3, 70.0,
70.0, 74.5, 72.1, 58.1, 44.6, 33.4, 28.6)
fit <- lm(Y~X)
coef <- round(fit$coef,3)
plot(X,Y)
abline(a=coef[1],b=coef[2],col='red',lwd=2)
text(55,10,paste0('Y=',coef[1],coef[2],'X'),cex=1.5)
```

The fitted linear model is Y=`r coef[1]` `r coef[2] `X


###The 2nd example should contains texts and at least one table. 
```{r,results = 'asis'}
library(xtable)
print(xtable(head(mtcars)),type="html")
```

The first five rows of the dataset "mtcars" are shown in the table above.The data was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).For example, the Mazda RX4's mpg(Miles/(US) gallon) is `r mtcars[1,1]`.



### The 3rd example should contain at least a couple of LaTeX formulas.

The initial-value problem is to solve the wave equation with the initial condinitions:
\begin{equation}
  \left\{\begin{array}{ll}
    \frac{\partial^{2} u}{\partial t^{2}}=a^{2} 
    \frac{\partial^{2} u}{\partial x^{2}}, & t>0,-\infty<x<+\infty \\
    \left.u\right|_{t=0}=\varphi(x), 
    & \left.\frac{\partial u}{\partial t}\right|_{t=0}=\psi(x)
  \end{array}\right.
\end{equation}

the solution is due to $\textbf{d'Alembert}$ in 1746:
\begin{equation}
  u(t, x)=\frac{1}{2}[\varphi(x-a t)+\varphi(x+a t)]
  +\frac{1}{2 a} \int_{x-a t}^{x+a t} \psi(\xi) \mathrm{d} \xi
\end{equation}

# HW1

## Exercise 3.3
* The Pareto($a,b$) distribution has cdf
$$
F(x)=1-\left(\frac{b}{x}\right)^{a}, \quad x \geq b>0, a>0
$$
  Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto$(2,2)$ distribution. Graph the density histogram of the sample with the Pareto$(2,2)$ density superimposed for comparison.

## Answer 

By the $\textbf{Inverse Transform Method}$, Let $U=F(x), U \sim Uniform(0,1)$, so we have
$$F(x)= U = 1-\left(\frac{b}{x}\right)^{a}$$
so we can get the probability inverse transformatioon $$ F^{-1}(U) =x = b*(1-U)^{-\frac{1}{a}}$$

For Pareto$(2,2)$, $$ F^{-1}(U) = 2*(1-U)^{-\frac{1}{2}}$$

```{r}
set.seed(303)
my.pareto <- function(a,b,n){
  u=runif(n)
  x=b*(1-u)^(-1/a)
  return(x)
}

x<-my.pareto(2,2,1000)#simulate a random sample from the Pareto(2, 2) distribution. 

#Graph the density histogram of the sample with the Pareto(2, 2) density superimposed for comparison.
hist(x, main = "Sample DensityHistogram vs Theoretical Density Curve for Pareto(2,2)", col = "green", prob =TRUE)
y<-seq(min(x),max(x),0.01)
lines(y,8/y^3,col="red")
  
```



## Exercise 3.9
*  The rescaled Epanechnikov kernel is a symmetric density function
$$
f_{e}(x)=\frac{3}{4}\left(1-x^{2}\right), \quad|x| \leq 1
$$
  Devroye and Györfi give the following algorithm for simulation from this distribution. Generate iid $U_{1}, U_{2}, U_{3} \sim$ Uniform$(-1,1)$. If $\left|U_{3}\right| \geq \left|U_{2}\right|$ and $\left|U_{3}\right| \geq\left|U_{1}\right|$, deliver $U_{2}$; otherwise deliver $U_{3}$. Write a function to generate random variates from $f_{e}$, and construct the histogram density estimate of a large simulated random sample.

## Answer
```{r}
set.seed(309)
my.epanechnikov <- function(n){
  x <- c()
  for(i in 1:n){
    u1 <- runif(1,-1,1)
    u2 <- runif(1,-1,1)
    u3 <- runif(1,-1,1)
    if(abs(u3) >= abs(u2) && abs(u3) >= abs(u1)) x[i] <- u2
    else x[i] <- u3
  }

  hist(x, main = "Sample Density Histogram vs Theoretical Density Curve", col = "green", prob =TRUE, xlim = c(-1, 1),ylim = c(0,1))
  
  domain <- seq(-1, 1, 0.01)
  f <- function(x){
    3/4 * (1 - x^2)
  }
  lines(domain, f(domain), col = "red") 
}

my.epanechnikov(1000)


```




## Exercise 3.10

*  Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_{e}$.

## Answer
Now we from another way to describe this algorithm: generate iid $X_{1},X_{2},X_{3} \sim Uniform(0,1)$, so we select one of the two smallest of $X_{i}$ randomly with the same probability, denoted as $X_{i}=X$.

For $0\leq t \leq 1$, the event $\left\{ X \leq t \right\}$ consists of two disjointed parts：

1.At least two of $X_{i}$ lie in $\left[0,t\right]$, the probability is given by:$$\mathrm{C}_3^2 t^{2} (1-t) + \mathrm{C}_3^3 t^{3} = t^{2} (3-2t)$$

2.Only one of the $X_{i}$ lies in $\left[0,t\right]$, the probability is given by:$$\mathrm{C}_3^1 t (1-t)^2 \times \frac{1}{2} = \frac{3}{2} t (1-t)^2$$

So the cdf is: $$F(t) = t^{2} (3-2t) + \frac{3}{2} t (1-t)^2 = \frac{1}{2} t (3-t^2)$$

and the density function is: $$f(t)=F'(t)=\frac{3}{2}(1-t^2)$$
Extending $f$ to the domain $\left[-1,1\right]$ by symmetry, then wo get $$f_{e}(t)=\frac{1}{2}f(t)=\frac{3}{4}(1-t^2)$$

## Exercise 3.13

*  It can be shown that the mixture in Exercise $3.12$ has a Pareto distribution with cdf
$$
F(y)=1-\left(\frac{\beta}{\beta+y}\right)^{r}, \quad y \geq 0
$$
 (This is an alternative parameterization of the Pareto cdf given in Exercise $3.3$.) Generate 1000 random observations from the mixture with $r=4$ and $\beta=2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

## Answer

By the $\textbf{Inverse Transform Method}$, let $U \sim Uniform(0,1)$. For $U=F(y)=1-\left(\frac{\beta}{\beta+y}\right)^{r}$, so we have $$y=F^{-1}(U)=\beta\left[(1-u)^{-\frac{1}{r}}-1\right]$$
```{r}
empirical_sample<-function(r,beta,n){
  u=runif(n)
  y=beta*((1-u)^(-1/r)-1)
  return(y)
}
x<-empirical_sample(r=4,beta=2,n=1000)
hist(x, main = "Sample Density Histogram vs Theoretical Density Curve", col = "green", prob =TRUE)
y<-seq(min(x),max(x),0.01)
lines(y,64/(2+y)^5,col="red")

```

# HW2

## Exercise 5.1

* **Compute a Monte Carlo estimate of $$\int_{0}^{\frac{\pi}{3}} sin(t) dt$$ and compare your estimate with the exact value of the integral.**


## Answer 

```{r}
set.seed(501)
n <- 1e4
x <- runif(n, min=0, max=pi/3)
theta.hat <- mean(sin(x)) * pi/3
theta.true<- -cos(pi/3) - (-cos(0))
print(c(theta.hat,theta.true))

```

We see that the Monte Carlo estimate (left) is within 0.46% of the true value (right). 

## Exercise 5.7

* **Refer to *Exercise 5.6*. $\theta$ = $\int_{0}^{1} e^{x}dx$. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from *Exercise 5.6*.**

## Answer
$\textbf{By the simple Monte Carlo method:}$ $\theta$ = $e^U, U \sim Uniform(0,1)$,  
$\theta$ can be estimated by 
$$\hat \theta=\frac{1}{m} \displaystyle\sum^{m}_{i=1} e^{U_i},  U_i \sim Uniform(0,1)$$
So the variance of $\hat \theta$ is  $$Var(\hat \theta)=\frac{1}{m} Var(e^U)$$,  
which can be estimated with $$\hat \sigma_m^2 = \frac{1}{m(m-1)} \displaystyle\sum^{m}_{i=1} \left( e^{U_i}-\frac{1}{m} \displaystyle\sum^{m}_{i=1} e^{U_i} \right)^2 $$  
Because $Var(e^U)$=  $E(e^{2U})-(E(e^U))^2$ = $\int_{0}^{1} e^{2x} dx$ - $[\int_{0}^{1} e^{x} dx]^2$ = $\frac{1}{2}(e^2-1) - (e-1)^2$, so the theoretical value of $Var(\hat \theta)$ is $\frac{1}{m}( \frac{e^2-1}{2} - (e-1)^2)$


$\textbf{By the antithetic variate approach:}$ the antithetic variable estimator of $\theta$ is
$$\hat \theta=\frac{1}{m} \displaystyle\sum^{m/2}_{i=1} \left(e^{U_i} + e^{1-U_i} \right),  U_i \sim Uniform(0,1)$$  

So the variance of $\hat \theta$ is  $$\begin{aligned} Var(\hat \theta) &=\frac{1}{m^2} Var \left[ \displaystyle\sum^{m/2}_{i=1} \left(e^{U_i} + e^{1-U_i} \right) \right] \\&= \frac{1}{m^2} \displaystyle\sum^{m/2}_{i=1}  \left[ Var (e^{U_i}) + Var(e^{1-U_i}) + 2 Cov(e^{U_i}, e^{1-U_i})\right] \end{aligned}$$,  

We have seen that the antithetic variables $U_i$ and $1–U_i$ are identically distributed, and so $e^{U_i}$ and $e^{1–U_i}$ must also be identically distributed. Hence $Var(e^{U_i}) = Var(e^{1–U_i})$ is a constant, say, $Var(e^U)$. Similarly, $Cov(e^{U_i}, e^{1-U_i})$ is a constant, say, $Cov(e^{U}, e^{1-U})$=$E(e^U e^{1-U}) - (Ee^U)(Ee^{1-U})$ = $e-(e-1)^2$. Thus we can write 
$$Var(\hat \theta)=\frac{1}{m^2} \displaystyle\sum^{m/2}_{i=1}  \left[ 2 Var (e^{U}) + 2 Cov(e^{U}, e^{1-U})\right] = \frac{1}{m} [Var (e^{U}) + Cov(e^{U}, e^{1-U})]$$  
So the theoretical value of $Var(\hat \theta)$ is $\frac{1}{m}[\frac{1}{2}(e^2-1)-(e-1)^2+e-(e-1)^2]$.  

The R code to mimic the analysis is:  

```{r}
set.seed(507)
m = 10000
u1 = runif( as.integer(m/2) )
u.anti = 1 - u1
g.anti = ( exp(u1) + exp(u.anti) )/2
Theta.anti = mean( g.anti )
var.anti = var( g.anti )/(m/2)
u = runif(m)
Theta.MC = mean( exp(u) )
var.MC = var( exp(u) )/m
perc.reduc = 100*(var.MC - var.anti)/var.MC

#Theoretical value
Theo.MC<-((exp(1)^2-1)/2 - (exp(1)-1)^2)/m
Theo.anti<-((exp(1)^2-1)/2-2*(exp(1)-1)^2+exp(1))/m
Theo.reduc<-100*(Theo.MC - Theo.anti)/Theo.MC

```

$\textbf{This produces the estimate of}$ $\theta$:
```{r}
cat("simple Monte Carlo method: ",Theta.MC, "\n", "antithetic variate approach: ",Theta.anti)
```
By Exercise 5.6, we know that the exact value of $\theta$ = $e-1$ = 1.718282

$\textbf{For the estimated variances we find:}$
```{r}
cat(" simple Monte Carlo method: ",var.MC, "\n", "antithetic variate approach: ", var.anti,"\n","the percent reduction in variance", perc.reduc,"%")
```

$\textbf{And the Variance results of the theoretical value from Exercise 5.6 is:}$
```{r}
cat(" Theoretical value by simple MC: ",Theo.MC, "\n", "Theoretical value by antithetic variate: ",Theo.anti,"\n","the percent reduction in variance", Theo.reduc,"%")
```



## Exercise 5.11

* **If $\hat \theta_{1}$ and $\hat \theta_{2}$ are unbiased estimators of $\theta$, and $\hat \theta_{1}$ and $\hat \theta_{2}$ are antithetic, we derived that $c^*$ = 1/2 is the optimal constant that minimizes the variance of $\hat \theta_{c} = c\hat \theta_{1} + (1 − c)\hat \theta_{2}$. Derive $c^*$ for the general case. That is, if $\hat \theta_{1}$ and $\hat \theta_{2}$ are any two unbiased estimators of $\theta$, find the value $c^*$ that minimizes the variance of the estimator $\hat \theta_{c} = c\hat \theta_{1} + (1 − c)\hat \theta_{2}$ in equation (5.11). ($c^*$ will be a function of the variances and the covariance of the estimators.)**

## Answer
Consider any unbiased estimator of the form $\hat \theta_{c}=c\hat \theta_{1} + (1 − c)\hat \theta_{2}=\hat \theta_{2}+c(\hat \theta_{1}-\hat \theta_{2})$,  
So the variance of $\hat \theta_{c}$ is:
$$\begin{aligned} 
Var(\hat \theta_{c}) &= Var(\hat \theta_{2})+c^2 Var(\hat \theta_{1}-\hat \theta_{2})+2c \cdot Cov(\hat \theta_{2}, \hat \theta{1}-\hat \theta_{2})) \\&= \left( c \text{ Sd}(\hat \theta_{1}-\hat \theta_{2})+\frac{Cov(\hat \theta_{2}, \hat \theta{1}-\hat \theta_{2})}{\text{Sd}(\hat \theta_{1}-\hat \theta_{2})}\right)^2 + Var(\hat \theta_2)- \frac{Cov^2(\hat \theta_2, \hat \theta_1-\hat \theta_2)}{Var(\hat \theta_1-\hat \theta_2)} \end{aligned}$$

 
$\textbf{In the special case of antithetic variates}$, $\hat{\theta}_{1}$ and $\hat{\theta}_{2}$ are identically distributed and $Cor\left(\hat{\theta}_{1}, \hat{\theta}_{2}\right)=-1$. Then $Cov\left(\hat{\theta}_{1},\hat{\theta}_{2}\right)=-Var\left(\hat{\theta}_{1}\right)$
and the variance of $\hat \theta_c$ is
$$
Var \hat{\theta}_{c}=4 c^{2} Var\left(\hat{\theta}_{1}\right)-4c Var\left(\hat{\theta}_{1}\right)+Var\left(\hat{\theta}_{1}\right)=\left(4 c^{2}-4c+1\right) Var\left(\hat{\theta}_{1}\right)
$$
and the optimal constant is $c^{*}=\frac{1}{2}$. The control variate estimator in this case is
$$
\hat{\theta}_{c^{*}}=\frac{\hat{\theta}_{1}+\hat{\theta}_{2}}{2}
$$

$\textbf{For general case}$, we can minimize the variance of the estimator $\hat \theta_{c}=c\hat \theta_{1} + (1 − c)\hat \theta_{2}$ When $$c=c^*=-\frac{Cov^2(\hat \theta_2, \hat \theta_1-\hat \theta_2)}{Var(\hat \theta_1-\hat \theta_2)}=\frac{Var(\hat\theta_2)-Cov(\hat\theta_1,\hat\theta_2)}{Var(\hat\theta_1)+Var(\hat\theta_2)-2Cov(\hat\theta_1,\hat\theta_2)}$$

# HW3

## Exercise 5.13

* **Find two importance functions $f_1$ and $f_2$ that are supported on $(1, \infty)$ and are ‘close’ to $$g(x)=\frac{x^2}{\sqrt{2 \pi}}e^{-x^2 /2},x>1.$$ Which of your two importance functions should produce the smaller variance in estimating$$\int_{1}^{\infty}\frac{x^2}{\sqrt{2 \pi}}e^{-x^2 /2}dx$$ by importance sampling? Explain.**


## Answer 
$(1):$We can notice that the change of variable $t=x^2$ over $x>1$ teakes $\theta$ to $$\theta=\int_1^\infty \frac{x^2}{\sqrt{2 \pi}}e^{-x^2 /2}dx=\int_1^\infty \frac{t^{1/2}}{2^{3/2}\sqrt{\pi}}e^{-t/2}dt$$
Choose $t\sim Gamma(\frac{3}{2},\frac{1}{2})$, the density function is: $$f(t)=\frac{t^{1/2}}{2^{3/2}\Gamma(\frac{3}{2})} e^{-\frac{t}{2}},t>0$$
So that $$\theta=\int_1^\infty \frac{\frac{t^{1/2}}{2^{3/2}\sqrt{\pi}}e^{-t/2}}{f(t)}f(t)dt=\int_1^\infty \frac{1}{2} f(t)dt=\frac{1}{2}P(t>1)$$

R code to mimic the analysis is:
```{r}
m<-1e6
t<-rgamma(m,shape = 3/2,scale = 2)
theta.hat<-0.5*mean(t>1)
se<-sd(0.5*(t>1))
cat("theta.hat: ",theta.hat,"\n","se of g/f:",se)
```

$(2):$Take $f$ is the density function of $N(0,1)$, i.e. $$f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},-\infty<x<\infty$$
So that $$\theta=\int_1^\infty \frac{ \frac{x^2}{\sqrt{2 \pi}}e^{-x^2 /2}}{f(x)}f(x)dx=\int_{-\infty}^\infty x^2 \textbf{1}_{[1,\infty)} f(x)dx$$

R code to mimic the analysis is:
```{r}
m<-1e6
x<-rnorm(m)
theta.hat<-mean(x^2*(x>1))
se<-sd(x^2*(x>1))
cat("theta.hat: ",theta.hat,"\n","se of g/f:",se)
```

So the importance function of $Gamma(\frac{3}{2},\frac{1}{2})$ produces the smaller variance in estimating $\theta$

## Exercise 5.15

* **Obtain the stratified importance sampling estimate in $Example 5.13$ and compare it with the result of $Example 5.10$.**

## Answer

In $Example 5.13$, we divide the interval(0,1) into five subintervals, $(j/5,(j+1)/5),j=0,1,...,4$. Then on the $j^{th}$ subinterval variables are generated form the density $$\frac{5e^{-x}}{1-e^{-1}}, \frac{j-1}{5}<x<\frac{j}{5}$$.  
The following code can obtain the stratified importance sampling estimate:
```{r}
M=1e4 #number of replicates
k<-5 #number of stratum
r=M/k #replicates per stratum
N=50 #number of times to repeat the estimation
T2<-numeric(k)
est<-matrix(0,N,2)

g<-function(x){
  exp(-x-log(1+x^2))*(x>0)*(x<1)
}
f<-function(x){
  exp(-x)/(1-exp(-1))
}

for(i in 1:N){
  est[i,1]<-mean(g(runif(M)))
  for(j in 1:k){
    u<-runif(r,(j-1)/k,j/k)
    x=-log(1-u*(1-exp(-1)))
    T2[j]<-mean(g(x)/(k*f(x)))
      }
  est[i,2]<-sum(T2)
}

apply(est,2,mean)

apply(est,2,sd)

```

The estimate of the intergration is close, while the estimated standard error is reduced by stratified importance sampling.


*Modify the density function of each subinterval:
```{r}
f_j<-function(x,j){
  exp(-x)/(exp(-(j-1)/5)-exp(-j/5))
}

theta.hat<-numeric(k)
se_j<-numeric(k)
for(j in 1:k){
  u<-runif(r,(j-1)/k,j/k)
  x=-log(exp(-(j-1)/5)-u*(exp(-(j-1)/5)-exp(-j/5)))
  theta.hat[j]<-mean(g(x)/f_j(x,j))
  se_j[j]<-sd(g(x)/f_j(x,j))
}

cat("theta.hat: ",sum(theta.hat),"\n","se:",sum(se_j))

```
In $Example 5.10$, the $theta.hat=0.52506988,se=0.09658794$.The simulation indicates that the of variance of the eatiamtor is reduced by "stratified importance sampling".




## Exercise 6.4

* **Suppose that $X_1,...,X_n$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a $95\%$ confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.**

## Answer
The following code runs a Monte Carlo simulation of using random lognormal variables to estimate the $\mu$ parameter and produce a confidence level.

```{r}
m <- 1000
n <- 2000
mu <- 1
sigma <- 0.5
alpha <- 0.05
res <- replicate(m, expr={
  x <- rlnorm(n, meanlog=mu, sdlog=sigma)
  log.mean <- mean(log(x))
  log.se <- sd(log(x)) / sqrt(n)
  t.val <- qt(1 - alpha / 2, n - 1)
  
  CI <- c(log.mean, log.mean - (t.val * log.se), log.mean + (t.val * log.se))
  CI
})
cat("an empirical estimate of 95% confidence interval for the parameter mu:" ,"\n", "(", mean(res[2,]), mean(res[3,]),")")

```

The calculation of the $95\%$ upper confidence limit (UCL) for a random sample size n = 2000 from a logNormal(1,0.5) distribution is shown below.

```{r}
UCL <- replicate(m, expr = {
  x <- rlnorm(n, meanlog=mu, sdlog=sigma)
  log.mean <- mean(log(x))
  log.se <- sd(log(x)) / sqrt(n)
  (log.mean -mu)/log.se
} )
CL<-mean(UCL > qt(alpha , n - 1))
CL
```
So the empirical confidence level is $`r CL`$ in this experiment.

## Exercise 6.5

* **Suppose a $95\%$ symmetric $t$-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size $n = 20$. Compare your $t$-interval results with the simulation results in Example 6.4. (The $t$-interval should be more robust to departures from normality than the interval for variance.)**

## Answer
Since $\frac{\bar{x}-\mu}{\sqrt{S_n/n}} \sim t(n-1)$, we can produce a t-interval about mean.The 95% t-interval is: $\bar{x}\pm\sqrt{\frac{S_n}{n}}*t_{1-\alpha/2}(n-1)$.   
Because this confidence interval(c.i) is based on the assumption that the sample is from normal distribution, the coverage probability of the c.i is not equal to $(1-\alpha)100\%$.The result from the lower code proves it.  
In this question, sample x is from $χ^2 (2)$. We can get the coverage probability of the t-interval is 0.908, which is lower than 0.95. Compared with the example 6.4, we can find the t-interval is more robust, because the coverage probability given by Chi-square method is 0.812, which is much smaller than 0.908.

```{r}
set.seed(123)
m<-1000
c<-numeric(m)
n<-20
for (i in 1:m) {
  x<-rchisq(n,2)
  alpha<-0.05
  u<-mean(x)+sqrt(var(x)/n)*qt(1-alpha/2,n-1)
  l<-mean(x)+sqrt(var(x)/n)*qt(alpha/2,n-1)
  if(2<=u&&2>=l) c[i]=1
}
cat("coverage probability(t-interval)=",sum(c)/m)
#The example using Chi-square interval
alpha <- 0.05
UCL <- replicate(m, expr = {
  x <- rchisq(n, df = 2)
  (n-1) * var(x) / qchisq(alpha, df = n-1)
} )
cat("\ncoverage probability(Chi-square interval)=",mean(UCL > 4))
```

# HW4

## Exercise 6.7

* **Estimate the power of the skewness test of normality against symmetric $\operatorname{Beta}(\alpha, \alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\nu) ?$**

## Answer
The skewness $\sqrt{b_1}$ of a random variable $X$ is defined by
\begin{equation}
  \sqrt{b_{1}} = 
    \frac{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{3}}
    {\left(\frac{1}{n}\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}\right)^{3 / 2}},
\end{equation}
which is asymptotically normal with mean $0$ and variance $6/n$ under normality assumption of $X$.

The skewness test of normality is done with hypotheses
\begin{equation}
H_0: \sqrt{\beta_1} = 0 \Leftrightarrow  H_1: \sqrt{\beta_1} \neq 0 .
\end{equation}
where the sampling distribution of the skewness statistic is derived under the assumption of normality.

Our goal is to estimate the power of the skewness test of normality against symmetric $Beta(\alpha, \alpha)$ distributions.
We just need to generate data from $Beta(\alpha, \alpha)$ distributions with some different values of $\alpha$ and then put then into the test machine as described above.

Here is the code:
```{r}
level<-0.1 # the signifiance level
n <- 30   # sample size
m <- 2500  # reputation times

# statistics function
skewness <- function(x){
  # input data x
  # return statistics b1
  x_bar <- mean(x)
  numerator <- mean((x-x_bar)^3)
  denominator <- (mean((x-x_bar)^2))^1.5
  return(numerator / denominator)
}

# critical value for the skewness test at level=0.1
cv <- qnorm(1-level/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))

# replicate the procedure with different parameter alpha
pwr <- function(a){
  stat <- replicate(m, expr={
                    X <- rbeta(n, a, a)
                    skewness(X) })
  length(which(abs(stat) >= cv)) / m
}

# calculate power
alpha <- seq(0.5, 40, 0.5)
powers <- unlist(base::lapply(X=alpha, FUN=pwr))

# plot power vs alpha
plot(alpha, powers, type = "b", xlab = bquote(alpha), ylab='Power', ylim = c(0,1))
# plot test level line, horizon
abline(h = .1, lty = 3)
# add standard errors
se <- sqrt(powers * (1-powers) / m) 
lines(alpha, powers+se, lty = 3,col="blue")
lines(alpha, powers-se, lty = 3,col="blue")
```

The power of the test procedure is really poor. 
It can hardly reject the hypotheses under such a kind of data $Beta(\alpha,\alpha), \alpha=0.5,1,\cdots,40$. 

To explain the result, I suppose when $\alpha$ is large, such as $20$, kurtosis is relatively large, which means the variance is small. At this time, $Beta(\alpha,\alpha)$ is similar with normal distribution.
On the contrary, when $\alpha$ is small, the variance is relatively small, but it's very small actually. So it's impossible for the test machine to reject the null hypothesis.

Now let's check the reusult of some other heavy-tailed symmetric alternatives such as $t(ν)$.

Here is the code for detail:
```{r}
df <- seq(1, 40) #degree of freedom for t-test
# replicate the procedure with different parameter alpha
pwr <- function(a){
  stat <- replicate(m, expr={
                    X <- rt(n, a)
                    skewness(X)})
  length(which(abs(stat) >= cv)) / m
}
powers <- unlist(base::lapply(X=df, FUN=pwr))
# plot power vs df
plot(df, powers, type = "b", xlab = 'Degree of Freedom', ylab='Power', ylim = c(0,1))
# plot test level line, horizon
abline(h = .1, lty = 3)
# add standard errors
se <- sqrt(powers * (1-powers) / m) 
lines(df, powers+se, lty = 3)
lines(df, powers-se, lty = 3)
```

We can see that the test procedure works well when the parameter of $t$ distribution, $df$, is relatively small. The power is about $0.9$ at the beginning, and it declines rapidly when $df$ becomes bigger. When the power is about $0.2$ and $df$ is about $8$, it slowed down the rate of decline and it slowly reduce to $0.1$, the level we selected, in the following process. 

I suppoese that, it is because $t$ distribution is different from normal distribution when $df$ is small, but when $df$ is larger they are similar. 


## Exercise 6.8

* **Refer to Example $6.16 .$ Repeat the simulation, but also compute the $F$ test of equal variance, at significance level $\hat{\alpha} = 0.055 .$ Compare the power of the Count Five test and $F$ test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.)**


## Answer 
```{r}
alpha = 0.055
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 1.5
sample_num <- c(15, 100, 1000)
m <- 1000
tests_F <- tests_CF <- numeric(length(sample_num))
testF <- function(x, y) {
  f_test <- var.test(x, y, alternative = "two.sided", conf.level = 1-alpha)
  return(f_test$p.value < alpha)
}
tests5 = function(x, y) {
  X<- x-mean(x)
  Y<- y-mean(y)
  out_x <- sum(X > max(Y)) + sum(X < min(Y))
  out_y <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(out_x, out_y)) > 5))
}

for (i in 1 : 3) {
  n <- sample_num[i]
  power_F <- mean(replicate(m, expr = {
    x <- rnorm(n, mu1, sigma1)
    y <- rnorm(n, mu2, sigma2)
    testF(x, y)
  }))
  power_CF <- mean(replicate(m, expr = {
    x <- rnorm(n, mu1, sigma1)
    y <- rnorm(n, mu2, sigma2)
    tests5(x, y)
  }))
  
  tests_F[i] <- power_F
  tests_CF[i] <- power_CF
}

#print the results
m<-matrix(c(tests_F, tests_CF),byrow=TRUE,nrow=2,
          dimnames = list(c("F test","Count five test"),c("size=15","size=100","size=1000")))
knitr::kable(m,align = "c",caption = "the power of the Count Five test vs F test")

```



## Exercise 6.C

* **Repeat Examples 6.8 and 6.10 for Mardia's multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are iid, the multivariate population skewness $\beta_{1, d}$ is defined by Mardia as$$\beta_{1, d}=E\left[(X-\mu)^{T} \Sigma^{-1}(Y-\mu)\right]^{3}$$Under normality, $\beta_{1, d}=0 .$ The multivariate skewness statistic is$$b_{1, d}=\frac{1}{n^{2}} \sum_{i, j=1}^{n}\left(\left(X_{i}-\bar{X}\right)^{T} \widehat{\Sigma}^{-1}\left(X_{j}-\bar{X}\right)\right)^{3}$$ where $\hat{\Sigma}$ is the maximum likelihood estimator of covariance. Large values of $b_{1, d}$ are significant. The asymptotic distribution of $n b_{1, d} / 6$ is chisquared with $d(d+1)(d+2) / 6$ degrees of freedom.**

## Answer
First write a function to compute the  multivariate skewness statistic :

```{r}
mardia_multskew<-function(x,s=var(x)){
  x<-as.matrix(x)
  n<-nrow(x)
  #d<-ncol(x)

  #creat 1 matrix
  one<-matrix(1,n)
  
  #creat idendity matrix n*n
  id<-matrix(0,n,n)
  diag(id)<-1
  
  #creat Q matrix (I-1/n * 1[n] * 1[n]' )
  Q <- id - 1/n * one %*% t(one)

  # create g matrix 
  g <- Q %*% x %*% solve(s) %*% t(x) %*% Q
  
  b1d <- 1/(n^2) * sum(g^3)# b1d is used for skew measure
  k1<-n*b1d/6
  return(k1)
}

```

* **repeat Examples 6.8**

```{r}
#repeat Examples 6.8
alpha<-0.05
n<-c(10,20,30,50,100) #sample size
d<-2
cv<-qchisq(1-alpha,df=d*(d+1)*(d+2)/6)#crit. values for chi-squared distribution

p.reject<-numeric(length(n))
m=1000

for (i in 1:length(n)) {
  skew<-numeric(m)
  for (j in 1:m) {
    data<-data.frame(x=rnorm(n[i]),y=rnorm(n[i]))
    #test decision is 1 (reject) or 0
    skew[j]<-as.integer(mardia_multskew(data)>=cv)
  }
  p.reject[i]<-mean(skew) #proportion rejected
}

p.reject

```
The results of the simulation suggest that the asymptotic normal approximation for the distribution of $n*b_{1,d}/6$ is not adequate for small sizes, and questionable for large size.



* **repeat Example 6.10**

```{r}
alpha<-0.1
n<-30
m=2500
eps<-c(seq(0,0.15,0.01),seq(0.15,1,0.05))
N<-length(eps)
pwr<-numeric(N)
cv<-qchisq(1-alpha,df=d*(d+1)*(d+2)/6)#crit. values for chi-squared distribution

for(j in 1:N){
  e<-eps[j]
  skew<-numeric(m)
  for (i in 1:m) {
    sigma<-sample(c(1,10),replace = TRUE,size = n,prob=c(1-e,e))
    data<-data.frame(rnorm(n,0,sigma),rnorm(n,0,sigma))
    skew[i]<-as.integer(mardia_multskew(data)>=cv)
  }
  pwr[j]<-mean(skew)
}


#plot power vs epsilon
plot(eps, pwr, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(eps, pwr+se, lty = 3)
lines(eps, pwr-se, lty = 3)

```




## Discussion
**If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?**

* **1.What is the corresponding hypothesis test problem?**


Denote $p_1$ and $p_2$ are powers of different method. The corresponding hypothesis test problem is :$$H_0: p_1 = p_2 \Leftrightarrow H_1: p_1 \neq p_2$$


* **2.What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test?**

**Z-test** is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large. Usually it is used when data size is large. 

**Two-sample t-test** is used when you want to compare two independent groups to see if their means are different with small data size. 

**Paired Samples T Test** compares the means of two variables. It computes the difference between the two variables for each case, and tests to see if the average difference is significantly different from zero. Requirement: Both variables should be normally distributed.

**McNemar's test** is a nonparameter statistical test used on paired nominal data. It's used when you are intersted in finding a change in proportion for the paired data.

We should not use **two-sample t-test** because it's used for independent sample with small sample size. Here the two samples are highly correlated and not independent.

And also, we cannot directly use *McNemar test* because we don't have paried data. If we have all details about 10000 experience, we can try it.


* **3.What information is needed to test your hypothesis?**

If we use *McNemar test* we need know the following information: 

$a$: the number of rejecting method-1 but accepting method-2 at the same time.

$b$: the number of accepting method-1 but rejecting method-2 at the same time.

The McNemar test formula is:
$$T = \frac{(a-b)^2}{a+b}.$$

We have $T \leadsto \chi^2(1)$.The critical value of $\chi^2(1)$ for $\alpha= 0.05$ is $3.84$. If $T>3.84$, then we can reject the null hypothesis and conclude that the powers are different $\alpha= 0.05$ level. 

# HW5

## Exercise 7.1

* **Compute a jackknife estimate of the bias and the standard error of the correlation statistic in $Example 7.2$.**

## Answer
For the law-school data from Example 7.2, sample R code is
```{r}
library(bootstrap); attach(law)
n = length( LSAT )
R.hat = cor( LSAT,GPA )
R.jack = numeric( n )
for (i in 1:n) { R.jack[i] = cor( LSAT[-i],GPA[-i] ) }
bias.jack = (n-1)*( mean(R.jack) - R.hat )
R.bar = mean(R.jack)
se.jack = sqrt( (n-1)*mean( (R.jack-R.bar)^2 ) ) 
detach(law)
cat("jackknife estimate for bias: ",bias.jack,"\n","jackknife estimate for se: ",se.jack)
```


## Exercise 7.5

* **Refer to $Exercise 7.4$. Compute $95\%$ bootstrap confidence intervals for the mean time between failures $1/ \lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.**

## Answer
```{r}
library(boot)
attach(aircondit)
x<-hours
detach(aircondit)
time.hat<-mean(x)  #MLE of 1/lambda
B=2000
set.seed(705)
time.boot<-function(x,i){
  mean(x[i])
}
boot.obj<-boot(data=x,statistic = time.boot,R=B)
print(boot.ci(boot.obj,type=c("norm","basic","perc","bca")))
```


   We see the intervals are quite disparate. A primary reason is that the bootstrap distribution is still skewed, affecting the simpler methods and their appeal to the Central Limit Theorem. 

   The BCa interval incorporates an acceleration adjustment for skew, and may be preferred here. And from the results, we can see the BCa interval is more credible, because it is transformation respecting and second order accurate , the percentile interval is transformation respecting but only first order accurate. The standard normal confidence interval is neither transformation respecting nor second order accurate. 


## Exercise 7.8

* **Refer to $Exercise 7.7$. Obtain the jackknife estimates of bias and standard error of $\hat \theta$.**


## Answer
```{r}
data(scor,package = "bootstrap")
lamda<-eigen(cov(scor))$values
theta.hat<-lamda[1]/sum(lamda)

#Jackknife
n=nrow(scor)
theta.jack<-numeric(n)
for (i in 1:n) {
  lambda.jack<-eigen(cov(scor[-i,]))$values
  theta.jack[i]<-lambda.jack[1]/sum(lambda.jack)
}
bias.jack<-(n-1)*(mean(theta.jack)-theta.hat)
se.jack<-sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2))

#print the results
cat("theta.hat: ",theta.hat,"\n","bias.jeck: ",bias.jack,"\n","se.jack: ",se.jack)
```


## Exercise 7.11

* **In $Example 7.18$, leave-one-out ($n$-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.**

## Answer

```{r}
library(DAAG); 
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)) # 'leave two out' has n(n-1) combinations
for (i in 1:n){
  for (j in i:n){
    if (i != j){
      y=magnetic[c(-i,-j)]
      x=chemical[c(-i,-j)]
      
      J1 <- lm(y ~ x)
      yhat11 <- J1$coef[1] + J1$coef[2] * chemical[i]
      yhat12 <- J1$coef[1] + J1$coef[2] * chemical[j]
      e1[(i-1)*n+j] <- sqrt((magnetic[i] - yhat11)^2+(magnetic[j] - yhat12)^2)
      
      J2 <- lm(y ~ x + I(x^2))
      yhat21 <- J2$coef[1] + J2$coef[2] * chemical[i] +
      J2$coef[3] * chemical[i]^2
      yhat22 <- J2$coef[1] + J2$coef[2] * chemical[j] +
      J2$coef[3] * chemical[j]^2
      e2[(i-1)*n+j] <- sqrt((magnetic[i] - yhat21)^2+(magnetic[j] - yhat22)^2)
      
      J3 <- lm(log(y) ~ x)
      logyhat31 <- J3$coef[1] + J3$coef[2] * chemical[i]
      logyhat32 <- J3$coef[1] + J3$coef[2] * chemical[j]
      yhat31 <- exp(logyhat31)
      yhat32 <- exp(logyhat32)
      e3[(i-1)*n+j] <- sqrt((magnetic[i] - yhat31)^2+(magnetic[j] - yhat32)^2)
      
      J4 <- lm(log(y) ~ log(x))
      logyhat41 <- J4$coef[1] + J4$coef[2] * log(chemical[i])
      logyhat42 <- J4$coef[1] + J4$coef[2] * log(chemical[j])
      yhat41 <- exp(logyhat41)
      yhat42 <- exp(logyhat42)
      e4[(i-1)*n+j] <- sqrt((magnetic[i] - yhat41)^2+(magnetic[j] - yhat42)^2)
    }
  }
}
detach(ironslag)
# estimates for prediction error
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))
```
According to the estimates for prediction error,the quadratic model is best fit for data. Then it's the exponential model, then the linear model. The Log-log model is the worst. 

The prediction error criterion of both methods shows that the quadratic model is best fit for the data. The fitted model is
$$\hat{Y} = 24.49262 - 1.39334X + 0.05452X^2$$

# HW6

## Exercise 8.3

* **The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.**
  
## Answer
```{r}
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0)
  return(as.integer(max(c(outx, outy)) > 5))
}
set.seed(123)
# Count Five test permutation
count5test_permutation <- function(z) {
  n <- length(z)
  x <- z[1:(n/2)]
  y <- z[-(1:(n/2))]
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y)) 
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  # return 1 (reject) or 0 (do not reject H0) 
  return(as.integer(max(c(outx, outy)) > 5))
}
permutation <- function(z,R) {
  n <- length(z)
  out <- numeric(R)
  for (r in 1: R){
    p <- sample(1:n ,n ,replace = FALSE)
    out[r] <- count5test_permutation(z[p])
  }
  sum(out)/R
}              
n1 <- 20
n2 <- 50
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 1e3
alphahat1 <- mean(replicate(m, expr={
  x <- rnorm(n1, mu1, sigma1)
  y <- rnorm(n2, mu2, sigma2)
  x <- x - mean(x) #centered by sample mean
  y <- y - mean(y)
  count5test(x, y)
}))
alphahat2 <- mean(replicate(m, expr={
  x <- rnorm(n1, mu1, sigma1)
  y <- rnorm(n2, mu2, sigma2)
  x <- x - mean(x) #centered by sample mean 
  y <- y - mean(y)
  z <- c(x,y)
  permutation(z,1000) 
})<0.05)
round(c(count5test=alphahat1,count5test_permutation=alphahat2),4)
```


By permutation, we can get the type-I error is 0.036, while without permutation, the type-I error is 0.318. Therefore, permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal performs better.





###Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations.
* **Unequal variances and equal expectations**
* **Unequal variances and unequal expectations**
* **Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)**
* **Unbalanced samples (say, 1 case versus 10 controls)**
* **Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8).**


####Unequal variances and equal expectations
In the following code, the first case, where Unbalanced samples are with unequal variances and equal expectations is discussed.

```{r}
library(RANN)
library(boot)
library(energy)
library(Ball)
library(survival)

m <- 30#times to loop
k<-3
p<-2#ncol
# mu <- 0.5
n1 <- n2 <- 50#nrow
R<-999#the number of replications in the bd.test function
n <- n1+n2
N = c(n1,n2)

Tn <- function(z, ix, sizes,k) {
  n1 <- sizes[1]
  n2 <- sizes[2]
  n <- n1 + n2
  if(is.vector(z)) z <- data.frame(z,0)
  z <- z[ix, ]
  NN <- nn2(data=z, k=k+1)
  block1 <- NN$nn.idx[1:n1,-1] 
  block2 <- NN$nn.idx[(n1+1):n,-1] 
  i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5) 
  (i1 + i2) / (k * n)
}


eqdist.nn <- function(z,sizes,k){#NN
  boot.obj <- boot(data=z,statistic=Tn,R=R,
  sim = "permutation", sizes = sizes,k=k)
  ts <- c(boot.obj$t0,boot.obj$t)
  p.value <- mean(ts>=ts[1])
  list(statistic=ts[1],p.value=p.value)
}

p.values <- matrix(NA,m,3)#to store p values

for(i in 1:m) {
  x <- matrix(rnorm(n1 * p, sd = 1), ncol = p)#unequal variances
  y <- matrix(rnorm(n2 * p, sd = 1.4), ncol = p)
  z <- rbind(x, y)
  p.values[i, 1] <- eqdist.nn(z, N, k)$p.value#NN
  p.values[i, 2] <- eqdist.etest(z, sizes = N, R = R)$p.value#in the energy package
  p.values[i, 3] <- bd.test(x = x, y = y, R = 999, seed = i)$p.value#"Ball Divergence" in the ball package
}

alpha <- 0.1#confidence level
power <- apply(p.values<alpha,2,mean)#compute the number of p.values which is less than 0.1 in each column
pow <- data.frame(methods = c('NN','energy','Ball'),power)
print(pow)
```


####Unequal variances and unequal expectations:
```{r}
for(i in 1:m) {
  x <- matrix(rnorm(n1 * p, mean = 0.4, sd = 1), ncol = p)#unequal variances and unequal expectations
  y <- matrix(rnorm(n2 * p, mean = 0, sd = 1.4), ncol = p)
  z <- rbind(x, y)
  p.values[i, 1] <- eqdist.nn(z, N, k)$p.value
  p.values[i, 2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i, 3] <- bd.test(x = x,  y = y,  R = 999,  seed = i)$p.value
}
alpha <- 0.1
power <- apply(p.values<alpha,2,mean)
pow <- data.frame(methods = c('NN','energy','Ball'),power)
print(pow)
```

####Non-normal distributions: t distribution with 1 df and bimodel distribution
```{r}
for(i in 1:m) {
  x <- matrix(rt(n1 * p,df = 1), ncol = p)# t distribution
  y <- matrix(rnorm(n2 * p,sd = sample(c(1,1.3),size = n2*p, prob = c(0.5,0.5),replace = T)), ncol = p)#bimodel distribution
  z <- rbind(x, y)
  p.values[i, 1] <- eqdist.nn(z, N, k)$p.value
  p.values[i, 2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i, 3] <- bd.test(x = x, y = y, R = 999, seed = i)$p.value
}
alpha <- 0.01
power <- apply(p.values<alpha,2,mean)
pow <- data.frame(methods = c('NN','energy','Ball'),power)
print(pow)
```

####Unbalanced samples
```{r}
n1 <- 50
n2 <- 5
n <- n1+n2
N = c(n1,n2)
for(i in 1:m) {
  x <- matrix(rnorm(n1*p,mean = 1), ncol = p)#100 samples
  y <- matrix(rnorm(n2*p,mean = 2), ncol = 2)#10 samples
  z <- rbind(x, y)
  p.values[i, 1] <- eqdist.nn(z, N, k)$p.value
  p.values[i, 2] <- eqdist.etest(z, sizes = N, R = R)$p.value
  p.values[i, 3] <- bd.test(x = x, y = y, R = 999, seed = i)$p.value
}
alpha <- 0.1
power <- apply(p.values<alpha,2,mean)
pow <- data.frame(methods = c('NN','energy','Ball'),power)
print(pow)
```

# HW7

## Exercise 9.4

* **Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.**


## Answer 
The standard Laplace distribution has density $f(x)=\frac{1}{2}e^{-|x|}$. 
Sample R-code for a random walk Metropolis sampler, using the proposal distribution $Y \sim N(X_t,\sigma^2)$ is: 
```{r}
f_laplace<-function(x){
  0.5*exp(-abs(x))
}

rw.Metropolis<-function(sig,x0,m){
  x<-numeric(m)
  x[1]<-x0
  u<-runif(m)
  k<-0
  for(i in 2:m){
    y<-rnorm(1,x[i-1],sig)
      if(u[i]<=(f_laplace(y)/f_laplace(x[i-1])))
        x[i]<-y else{
          x[i]<-x[i-1]
          k<-k+1
        }
  }
  return(list(x=x,k=k))
}
```

Mimicking the approach in Example 9.3, set m=2000 and $\sigma$ over 0.05, 0.5, 2, 16. Use an arbitary initial point of $X_0$=25. Then, find the corresponding four chains via the sample R code:
```{r}
m=2000
sig<-c(0.05,0.5,2,16)
x0<-25
rw1<-rw.Metropolis(sig[1],x0,m)
rw2<-rw.Metropolis(sig[2],x0,m)
rw3<-rw.Metropolis(sig[3],x0,m)
rw4<-rw.Metropolis(sig[4],x0,m)
accept<-1-c(rw1$k/m,rw2$k/m,rw3$k/m,rw4$k/m)  #acceptance rates
results<-rbind(sig,accept)
colnames(results)<-c("chain1","chain2","chain3","chain4")
print(results)
```
Only the third chain has a rejection rate in the range [0.15,0.5]. The plots below show that the random walk Metropolissampler is very sensitive to the variance of the proposal distribution:
```{r}
rw<-cbind(rw1$x,rw2$x,rw3$x,rw4$x)
for(i in 1:4){
  plot(rw[,i],type="l",xlab=bquote(sigma==.(round(sig[i],3))),ylab="X",ylim=range(rw[,i]))
}
```


## Exercise 9.4(2)

* **For Exercise 9.4, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat R<1.2$**

## Answer

```{r}
Gelman_Rubin<-function(psi){
  psi<-as.matrix(psi) #psi[i,j] is the statistic psi(X[i,1:j]) for the chain in i-th row of X
  n<-ncol(psi)
  k<-nrow(psi)
  psi.mean<-rowMeans(psi)
  B<-n*var(psi.mean) #between variance est
  psi.w<-apply(psi,1,"var") #within variance
  W<-mean(psi.w) #within est
  v.hat<-W*(n-1)/n+(B/n) #upper variance est
  r.hat<-v.hat/W #G-R statistic
  return(r.hat)
}

sigma <- 2 #parameter of proposal distribution
n=15000 #length of chains
k <- 4 #number of chains to generate
b <- 1000 #burn-in length
x0<-c(-15,-5,15,25)


my_chains<-function(sig,x0,m){
  x<-numeric(m)
  x[1]<-x0
  u<-runif(m)
  for(i in 2:m){
    y<-rnorm(1,x[i-1],sig)
      if(u[i]<=(f_laplace(y)/f_laplace(x[i-1])))
        x[i]<-y else{
          x[i]<-x[i-1]
        }
  }
  return(x)
}

#generate the chains
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k)
X[i, ] <- my_chains(sigma,x0[i],n)

#compute diagnostic statistics
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
print(Gelman_Rubin(psi))

#plot psi for the four chains
for (i in 1:k)
plot(psi[i, (b+1):n], type="l",xlab=i, ylab=bquote(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman_Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.1, lty=2)


```


## Exercise 11.4

* **Find the intersection points $A(k)$ in $(0, \sqrt{k})$ of the curves$$S_{k-1}(a)=P\left(t(k-1)>\sqrt{\frac{a^{2}(k-1)}{k-a^{2}}}\right)$$and$$S_{k}(a)=P\left(t(k)>\sqrt{\frac{a^{2} k}{k+1-a^{2}}}\right),$$for $k=4: 25,100,500,1000,$ where $t(k)$ is a Student $t$ random variable with $k$ degrees of freedom. (These intersection points determine the critical values for a $t$ -test for scale-mixture errors proposed by Székely [260] .)**

## Answer
The upper-tail t-probabilities are found via the $pt()$ function with the $lower.tail=F$ option. The intersection of the two upper-tail functions $S_{k–1}(a)$ and $S_k(a)$ occurs when their difference is zero. A quick plot of selected differences $S_k(a) – S_{k–1}(a)$ shows that the likely solutions occur in the interval $1 < a < 2$, so use this as the capture interval for application of Brent’s method. Sample R code:

```{r}
k<-c(4:25,100,500,1000)

object<-function(a,df){
  a2<-a^2
  arg<-sqrt(a2*df/(df+1-a2))
  sk<-pt(q=arg,df=df,lower=F)
  
  arg=sqrt(a2*(df-1)/(df-a2))
  skm1<-pt(q=arg,df=df-1,lower=F)
  
  return(sk-skm1)
}

for(i in 1:length(k)){
  print(c(as.integer(k[i]),uniroot(object,lower=1,upper=2,df=k[i])$root))
  
}

```

# HW8

## A-B-O blood type problem 
* Let the three alleles be $A, B,$ and $O$. 

$$
\begin{array}{l|l|l|l|l|l|l|l}
\hline \text { Genotype } & \text { AA } & \text { BB } & \text { OO } & \text { AO } & \text { BO } & \text { AB } & \text { Sum } \\
\hline \text { Frequency } & \mathrm{p}^2 & \mathrm{q}^2 & \mathrm{r}^2 & 2 \mathrm{pr} & 2 \mathrm{qr} & 2 \mathrm{pq} & 1 \\
\hline \text { Count } & \mathrm{nAA} & \mathrm{nBB} & \mathrm{nOO} & \mathrm{nAO} & \mathrm{nBO} & \mathrm{nAB} & \mathrm{n} \\
\hline
\end{array}
$$

* Observed data: $n_{A \cdot}=n_{A A}+n_{A O}=444$ (A-type), $n_{B \cdot}=n_{B B}+n_{B O}=132$ (B-type), $n_{OO}=361$(O-type), $n_{A B}=63$(AB-type).
* Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{A A}$ and $n_{B B}$ ). 
* Record the values of $p$ and $q$ that maximize the conditional likelihood in each EM steps, calculate the corresponding log-maximum likelihood values (for observed data), are they increasing?


## Answer 
log-likelihood for complete data:
$$l_c(p)= n_{A A} \log (p^{2})+n_{A O} \log(2pr)+n_{B B} \log (q^{2})+n_{B O} \log \left(2 qr\right)+n_{A B} \log(pq)+n_{OO} \log(r^{2})$$
E-step:
$$
E[l_c(p)|n_{A \cdot},n_{B \cdot},n_{AB},n_{OO}]= n_{A A}^{(t)} \log \left(p^{2}\right)+n_{A O}^{(t)} \log \left(2 pr\right)+n_{B B}^{(t)} \log (q^{2})+n_{B O}^{(t)} \log \left(2 qr\right) + n_{A B}^{(t)} \log \left(pq\right)+n_{O O}^{(t)} \log \left(r^{2}\right)
$$
In the above formula, the partial derivatives of $p$ and $q$ are calculated respectively: (note that r=1-p-q)

$$\frac{\partial E}{\partial p}=\frac{2 n_{A A}^{(t)}+n_{A O}^{(t)}+n_{A B}^{(t)}}{p}-\frac{2 n_{O O}^{(t)}+n_{A O}^{(t)}+n_{B O}^{(t)}}{1-p-q}=0$$

$$\frac{\partial E}{\partial q}=\frac{2 n_{B B}^{(t)}+n_{B O}^{(t)}+n_{A B}^{(t)}}{q}-\frac{2 n_{O O}^{(t)}+n_{A O}^{(t)}+n_{B O}^{(t)}}{1-p-q}=0$$
M-step: we get :
$$p^{(\mathrm{t}+1)}=\frac{2 \mathrm{n}_{\mathrm{AA}}^{(\mathrm{t})}+\mathrm{n}_{\mathrm{AO}}^{(\mathrm{t})}+\mathrm{n}_{\mathrm{AB}}^{(\mathrm{t})}}{2 \mathrm{n}}$$
$$q^{(\mathrm{t}+1)}=\frac{2 \mathrm{n}_{\mathrm{BB}}^{(\mathrm{t})}+\mathrm{n}_{\mathrm{BO}}^{(\mathrm{t})}+\mathrm{n}_{\mathrm{AB}}^{(\mathrm{t})}}{2 \mathrm{n}}$$
$$r^{(\mathrm{t}+1)}=\frac{2 \mathrm{n}_{\mathrm{OO}}^{(\mathrm{t})}+\mathrm{n}_{\mathrm{AO}}^{(\mathrm{t})}+\mathrm{n}_{\mathrm{BO}}^{(\mathrm{t})}}{2 \mathrm{n}}$$
Here is the code to sumulate the above process:

```{r}
blood<-function(p,q,na,nb,nab,noo,N){
  n<-sum(na,nb,nab,noo)
  pt<-qt<-rt<-l_obs<-numeric(0)
  r<-1-p-q
  l_obs[1]<-na*log(p^2+2*p*r)+nb*log(q^2+2*q*r)+noo*log(r^2)+nab*log(2*p*q)
  
  pt[1]<-p
  qt[1]<-q
  rt[1]<-1-p-q
  for(i in 1:N){
    #E-step
    p.old<-pt[i]
    q.old<-qt[i]
    r.old<-rt[i]
    
    naa_t<-na*p.old^2/(p.old^2+2*p.old*r.old)
    nao_t<-na*2*p.old*r.old/(p.old^2+2*p.old*r.old)
    nbb_t<-nb*q.old^2/(q.old^2+2*q.old*r.old)
    nbo_t<-nb*2*q.old*r.old/(q.old^2+2*q.old*r.old)
    
    #M-step: update p,q,r
    pt[i+1]<-(2*naa_t+nao_t+nab)/(2*n)
    qt[i+1]<-(2*nbb_t+nbo_t+nab)/(2*n)
    rt[i+1]<-(2*noo+nao_t+nbo_t)/(2*n)
  
    # calculate the corresponding log-maximum likelihood values (for observed data)
    l_obs[i+1]<-na*log(pt[i+1]^2+2*pt[i+1]*rt[i+1])+nb*log(qt[i+1]^2+2*qt[i+1]*rt[i+1])+noo*log(rt[i+1]^2)+nab*log(2*pt[i+1]*qt[i+1])
    
    
    if(abs(pt[i+1]-p.old)/p.old < tol) break
  }
  return(list(pt=pt,qt=qt,rt=rt,l_obs=l_obs))
}


tol <- .Machine$double.eps^0.5
na<-444;nb<-132;noo<-361;nab<-63
N <- 1000 #max. number of iterations
p<-1/3;q<-1/3  #initial est. for p,q
my_EM_results<-blood(p,q,na,nb,nab,noo,N)
p_t<-my_EM_results$pt;q_t<-my_EM_results$qt;
r_t<-my_EM_results$rt;l_obs<-my_EM_results$l_obs
print(cbind(p_t,q_t,r_t,l_obs))  
plot(1:length(l_obs), l_obs, "b", xlab="Iter. number", main="Log-likelihood for observed data")

```

The values of $p$ and $q$ that maximize the conditional likelihood in each EM steps are shown above. By calculating the corresponding log-maximum likelihood values (for observed data), we can see that they are increasing.


## Exercises 3 (page 204, Advanced R)

* **Use both for loops and $lapply()$ to fit linear models to the mtcars using the formulas stored in this list:**

```{r}
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
```


## Answer 
```{r}
set.seed(1)
attach(mtcars)
```

### loop
```{r}
# loop
for (x in formulas){
   print(lm(x))
}
```

### lapply
```{r}
# lapply
lapply(formulas, lm)
detach()
```


## Excecises 3(page 213, Advanced R)

* **The following code simulates the performance of a t-test for non-normal data. Use $sapply()$ and an anonymous function to extract the p-value from every trial.**
```{r}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
```
**Extra challenge: get rid of the anonymous function by using [[ directly.**

## Answer
```{r}
sapply(trials,function(x){
  return(round(x$p.value,4))
})
```

```{r}
##get rid of the anonymous function
round(sapply(trials, "[[", "p.value"),4)
```





## Exercises 6(page 214, Advanced R)
* **Implement a combination of $Map()$ and $vapply()$ to create an $lapply()$ variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?**

## Answer

```{r}
library(parallel)
mcvMap <- function(f, FUN.VALUE , ...) {
    out <- mcMap(f, ...)
    vapply(out, identity, FUN.VALUE)
}
```


sorry, I don't understand this questional. So I put my another answer follow:
```{r}
data <- matrix(rnorm(20, 0, 10), nrow = 4)
x <- as.data.frame(data)
answer1 <- Map("/",x,vapply(x,mean,c(1)))
answer2 <- lapply(x,function(data){data/(mean(data))})
print(data.frame(unlist(answer1),unlist(answer2)))
```

# HW9

## Exercise 9.4
Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

* **Write an Rcpp function for Exercise 9.4 (page 277, Statistical Computing with R).**
  
* **Compare the corresponding generated random numbers with those by the R function you wrote before using the function "qqplot".**

* **Compare the computation time of the two functions with the function "microbenchmark".**

* **Comments your results.**


## Answer

## define cpp function and R function

```{r}
library(Rcpp)
library(microbenchmark)

set.seed(123)
#define cpp function
cppFunction('List rwMC(double sigma, double x0, int N) {
     NumericVector x(N);
     x[0]=x0;
     auto u = runif(N);
     int k = 0;
     for (int i = 1;i < N; i++) {
         auto y = rnorm(1, x[i - 1], sigma)[0];
         if (u[i] <= (exp(-abs(y))/exp(-abs(x[i-1])))) {
             x(i) = y;
         } else {
             x[i] = x[i-1];
             k++;
         }
     }
     return List::create(Named("x") = x, Named("k") = k);
 }')


#define R function
rwMR <- function(sigma, x0, N){
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (exp(-abs(y)) / exp(-abs(x[i-1])))) x[i] = y 
    else {
      x[i] = x[i-1]
      k = k+1
    }
  }
  return(list(x = x, k = k))
}
```

## the plot with sigma=0.5
```{r}
out1<-rwMC(0.5,25,2000)
out2<-rwMR(0.5,25,2000)
par(mfrow=c(1,2))
plot(out1$x, type="l", main='rwMC(sigma=0.5)', ylab="x",xlab = "")
plot(out2$x,type="l", main='rwMR(sigma=0.5)', ylab="x",xlab = "")
par(mfrow=c(1,1))
#compare the generated random numbers
qqplot(out1$x,out2$x,xlab = 'rwMR',ylab = 'rwMC',main='qqplot of two functions(sigma=0.5)')
abline(a=0,b=1)
#compare the computation time
ts <- microbenchmark(r=rwMR(0.5,25,2000),cpp=rwMC(0.5,25,2000))
summary(ts)
```

## the plot with sigma=2
```{r}
out1<-rwMC(2,25,2000)
out2<-rwMR(2,25,2000)
par(mfrow=c(1,2))
plot(out1$x, type="l", main='rwMC(sigma=2)', ylab="x",xlab = "")
plot(out2$x,type="l", main='rwMR(sigma=2)', ylab="x",xlab = "")
par(mfrow=c(1,1))
#compare the generated random numbers
qqplot(out1$x,out2$x,xlab = 'rwMR',ylab = 'rwMC',main='qqplot of two functions(sigma=2)')
abline(a=0,b=1)
#compare the computation time
ts <- microbenchmark(r=rwMR(2,25,2000),cpp=rwMC(2,25,2000))
summary(ts)
```

## the plot with sigma=10
```{r}
out1<-rwMC(10,25,2000)
out2<-rwMR(10,25,2000)
par(mfrow=c(1,2))
plot(out1$x, type="l", main='rwMC(sigma=10)', ylab="x",xlab = "")
plot(out2$x,type="l", main='rwMR(sigma=10)', ylab="x",xlab = "")
par(mfrow=c(1,1))
#compare the generated random numbers
qqplot(out1$x,out2$x,xlab = 'rwMR',ylab = 'rwMC',main='qqplot of two functions(sigma=10)')
abline(a=0,b=1)
#compare the computation time
ts <- microbenchmark(r=rwMR(10,25,2000),cpp=rwMC(10,25,2000))
summary(ts)
```



## the plot with sigma=16
```{r}
out1<-rwMC(16,25,2000)
out2<-rwMR(16,25,2000)
par(mfrow=c(1,2))
plot(out1$x, type="l", main='rwMC(sigma=16)', ylab="x",xlab = "")
plot(out2$x,type="l", main='rwMR(sigma=16)', ylab="x",xlab = "")
par(mfrow=c(1,1))
#compare the generated random numbers
qqplot(out1$x,out2$x,xlab = 'rwMR',ylab = 'rwMC',main='qqplot of two functions(sigma=16)')
abline(a=0,b=1)
#compare the computation time
ts <- microbenchmark(r=rwMR(16,25,2000),cpp=rwMC(16,25,2000))
summary(ts)
```

## Analysis
Both functions are showed in the code above. By compare the results of two functions and their "qqplot", we can find that the chains generated by two functions are similar when the $\sigma$ is appropriate. (while $\sigma$ is small, the chain is not good and the chains generated by two functions may different). However, compare the computing time, we can easily find that the cpp function is more efficient than R function under different sigma. Thus, The cpp function performs better than R function in this case.  


